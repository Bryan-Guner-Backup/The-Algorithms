{
  "slug": "softmax",
  "name": "Softmax",
  "categories": ["math"],
  "body": {},
  "implementations": {
    "python": {
      "dir": "maths/softmax.py",
      "url": "https://github.com/TheAlgorithms/python/tree/master/maths/softmax.py",
      "code": "<span class=\"hljs-string\">&quot;&quot;&quot;\nThis script demonstrates the implementation of the Softmax function.\n\nIts a function that takes as input a vector of K real numbers, and normalizes\nit into a probability distribution consisting of K probabilities proportional\nto the exponentials of the input numbers. After softmax, the elements of the\nvector always sum up to 1.\n\nScript inspired from its corresponding Wikipedia article\nhttps://en.wikipedia.org/wiki/Softmax_function\n&quot;&quot;&quot;</span>\n\n<span class=\"hljs-keyword\">import</span> numpy <span class=\"hljs-keyword\">as</span> np\n\n\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">softmax</span>(<span class=\"hljs-params\">vector</span>):\n    <span class=\"hljs-string\">&quot;&quot;&quot;\n    Implements the softmax function\n\n    Parameters:\n        vector (np.array,list,tuple): A  numpy array of shape (1,n)\n        consisting of real values or a similar list,tuple\n\n\n    Returns:\n        softmax_vec (np.array): The input numpy array  after applying\n        softmax.\n\n    The softmax vector adds up to one. We need to ceil to mitigate for\n    precision\n    &gt;&gt;&gt; np.ceil(np.sum(softmax([1,2,3,4])))\n    1.0\n\n    &gt;&gt;&gt; vec = np.array([5,5])\n    &gt;&gt;&gt; softmax(vec)\n    array([0.5, 0.5])\n\n    &gt;&gt;&gt; softmax([0])\n    array([1.])\n    &quot;&quot;&quot;</span>\n\n    <span class=\"hljs-comment\"># Calculate e^x for each x in your vector where e is Euler&#x27;s</span>\n    <span class=\"hljs-comment\"># number (approximately 2.718)</span>\n    exponentVector = np.exp(vector)\n\n    <span class=\"hljs-comment\"># Add up the all the exponentials</span>\n    sumOfExponents = np.<span class=\"hljs-built_in\">sum</span>(exponentVector)\n\n    <span class=\"hljs-comment\"># Divide every exponent by the sum of all exponents</span>\n    softmax_vector = exponentVector / sumOfExponents\n\n    <span class=\"hljs-keyword\">return</span> softmax_vector\n\n\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">&quot;__main__&quot;</span>:\n    <span class=\"hljs-built_in\">print</span>(softmax((<span class=\"hljs-number\">0</span>,)))\n"
    },
    "javascript": {
      "dir": "Maths/Softmax.js",
      "url": "https://github.com/TheAlgorithms/javascript/tree/master/Maths/Softmax.js",
      "code": "<span class=\"hljs-comment\">// Wikipedia: https://en.wikipedia.org/wiki/Softmax_function</span>\n\n<span class=\"hljs-keyword\">const</span> <span class=\"hljs-title function_\">Softmax</span> = (<span class=\"hljs-params\">inputs</span>) =&gt; {\n  <span class=\"hljs-keyword\">const</span> eulerExpOfAllInputs = inputs.<span class=\"hljs-title function_\">map</span>(<span class=\"hljs-function\"><span class=\"hljs-params\">input</span> =&gt;</span> <span class=\"hljs-title class_\">Math</span>.<span class=\"hljs-title function_\">exp</span>(input))\n  <span class=\"hljs-keyword\">const</span> sumOfEulerExpOfAllInputs = eulerExpOfAllInputs.<span class=\"hljs-title function_\">reduce</span>(<span class=\"hljs-function\">(<span class=\"hljs-params\">a, b</span>) =&gt;</span> a + b)\n\n  <span class=\"hljs-keyword\">return</span> inputs.<span class=\"hljs-title function_\">map</span>(<span class=\"hljs-function\">(<span class=\"hljs-params\">input</span>) =&gt;</span> {\n    <span class=\"hljs-keyword\">const</span> eulerExpInputs = <span class=\"hljs-title class_\">Math</span>.<span class=\"hljs-title function_\">exp</span>(input)\n    <span class=\"hljs-keyword\">return</span> eulerExpInputs / sumOfEulerExpOfAllInputs\n  })\n}\n\n<span class=\"hljs-keyword\">export</span> { <span class=\"hljs-title class_\">Softmax</span> }\n"
    },
    "matlab-octave": {
      "dir": "algorithms/machine_learning/Activation Functions/Softmax.m",
      "url": "https://github.com/TheAlgorithms/matlab-octave/tree/master/algorithms/machine_learning/Activation Functions/Softmax.m",
      "code": "<span class=\"hljs-comment\">%Softmax function</span>\nsyms k\nx = <span class=\"hljs-number\">-10</span>:<span class=\"hljs-number\">1</span>:<span class=\"hljs-number\">10</span>;\ndenom = symsum(<span class=\"hljs-built_in\">exp</span>(k),k,<span class=\"hljs-number\">-10</span>,<span class=\"hljs-number\">10</span>);\ny= @(x) (<span class=\"hljs-built_in\">exp</span>(x)/denom);\nfplot(y);\nxlabel(<span class=\"hljs-string\">&#x27;x&#x27;</span>);\nylabel(<span class=\"hljs-string\">&#x27;y&#x27;</span>);\ngrid on\n"
    }
  },
  "contributors": [
    {
      "name": "Yukti Khosla",
      "email": "44090430+Yukti-09@users.noreply.github.com",
      "commits": 1
    },
    {
      "name": "Kaushik Amar Das",
      "email": "cozek@users.noreply.github.com",
      "commits": 1
    },
    {
      "name": "shellhub",
      "email": "shellhub.me@gmail.com",
      "commits": 1
    },
    {
      "name": "Fergus McDonald",
      "email": "fergus@procurify.com",
      "commits": 2
    }
  ],
  "explanationUrl": {}
}
