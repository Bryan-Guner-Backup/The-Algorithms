{
  "slug": "relu",
  "name": "Relu",
  "categories": ["math"],
  "body": {},
  "implementations": {
    "python": {
      "dir": "maths/relu.py",
      "url": "https://github.com/TheAlgorithms/python/tree/master/maths/relu.py",
      "code": "<span class=\"hljs-string\">&quot;&quot;&quot;\nThis script demonstrates the implementation of the ReLU function.\n\nIt&#x27;s a kind of activation function defined as the positive part of its argument in the\ncontext of neural network.\nThe function takes a vector of K real numbers as input and then argmax(x, 0).\nAfter through ReLU, the element of the vector always 0 or real number.\n\nScript inspired from its corresponding Wikipedia article\nhttps://en.wikipedia.org/wiki/Rectifier_(neural_networks)\n&quot;&quot;&quot;</span>\n<span class=\"hljs-keyword\">from</span> __future__ <span class=\"hljs-keyword\">import</span> annotations\n\n<span class=\"hljs-keyword\">import</span> numpy <span class=\"hljs-keyword\">as</span> np\n\n\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">relu</span>(<span class=\"hljs-params\">vector: <span class=\"hljs-built_in\">list</span>[<span class=\"hljs-built_in\">float</span>]</span>):\n    <span class=\"hljs-string\">&quot;&quot;&quot;\n    Implements the relu function\n\n    Parameters:\n        vector (np.array,list,tuple): A  numpy array of shape (1,n)\n        consisting of real values or a similar list,tuple\n\n\n    Returns:\n        relu_vec (np.array): The input numpy array, after applying\n        relu.\n\n    &gt;&gt;&gt; vec = np.array([-1, 0, 5])\n    &gt;&gt;&gt; relu(vec)\n    array([0, 0, 5])\n    &quot;&quot;&quot;</span>\n\n    <span class=\"hljs-comment\"># compare two arrays and then return element-wise maxima.</span>\n    <span class=\"hljs-keyword\">return</span> np.maximum(<span class=\"hljs-number\">0</span>, vector)\n\n\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">&quot;__main__&quot;</span>:\n    <span class=\"hljs-built_in\">print</span>(np.array(relu([-<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">0</span>, <span class=\"hljs-number\">5</span>])))  <span class=\"hljs-comment\"># --&gt; [0, 0, 5]</span>\n"
    },
    "matlab-octave": {
      "dir": "algorithms/machine_learning/Activation Functions/ReLU.m",
      "url": "https://github.com/TheAlgorithms/matlab-octave/tree/master/algorithms/machine_learning/Activation Functions/ReLU.m",
      "code": "\n<span class=\"hljs-comment\">%ReLU function</span>\n\nx = <span class=\"hljs-number\">-10</span>:<span class=\"hljs-number\">0.01</span>:<span class=\"hljs-number\">10</span>;\ny = x;\ny(x&lt;<span class=\"hljs-number\">0</span>) = <span class=\"hljs-number\">0</span>;\n<span class=\"hljs-built_in\">plot</span>(x,y);\nxlabel(<span class=\"hljs-string\">&#x27;x&#x27;</span>);\nylabel(<span class=\"hljs-string\">&#x27;y&#x27;</span>);\ngrid on\n\n"
    }
  },
  "contributors": [
    {
      "name": "shellhub",
      "email": "shellhub.me@gmail.com",
      "commits": 1
    },
    {
      "name": "yoshitaka-i",
      "email": "8393063+inoue0426@users.noreply.github.com",
      "commits": 1
    },
    {
      "name": "Yukti Khosla",
      "email": "44090430+Yukti-09@users.noreply.github.com",
      "commits": 2
    },
    {
      "name": "Christian Clauss",
      "email": "cclauss@me.com",
      "commits": 3
    }
  ],
  "explanationUrl": {}
}
