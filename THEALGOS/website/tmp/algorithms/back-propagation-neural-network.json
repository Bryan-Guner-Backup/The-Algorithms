{
  "slug": "back-propagation-neural-network",
  "name": "Back Propagation Neural Network",
  "categories": ["neuralnetwork"],
  "body": {},
  "implementations": {
    "python": {
      "dir": "neural_network/back_propagation_neural_network.py",
      "url": "https://github.com/TheAlgorithms/python/tree/master/neural_network/back_propagation_neural_network.py",
      "code": "<span class=\"hljs-comment\">#!/usr/bin/python</span>\n\n<span class=\"hljs-string\">&quot;&quot;&quot;\n\nA Framework of Back Propagation Neural Network（BP） model\n\nEasy to use:\n    * add many layers as you want ！！！\n    * clearly see how the loss decreasing\nEasy to expand:\n    * more activation functions\n    * more loss functions\n    * more optimization method\n\nAuthor: Stephen Lee\nGithub : https://github.com/RiptideBo\nDate: 2017.11.23\n\n&quot;&quot;&quot;</span>\n<span class=\"hljs-keyword\">import</span> numpy <span class=\"hljs-keyword\">as</span> np\n<span class=\"hljs-keyword\">from</span> matplotlib <span class=\"hljs-keyword\">import</span> pyplot <span class=\"hljs-keyword\">as</span> plt\n\n\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">sigmoid</span>(<span class=\"hljs-params\">x</span>):\n    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-number\">1</span> / (<span class=\"hljs-number\">1</span> + np.exp(-<span class=\"hljs-number\">1</span> * x))\n\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">DenseLayer</span>:\n    <span class=\"hljs-string\">&quot;&quot;&quot;\n    Layers of BP neural network\n    &quot;&quot;&quot;</span>\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\">\n        self, units, activation=<span class=\"hljs-literal\">None</span>, learning_rate=<span class=\"hljs-literal\">None</span>, is_input_layer=<span class=\"hljs-literal\">False</span>\n    </span>):\n        <span class=\"hljs-string\">&quot;&quot;&quot;\n        common connected layer of bp network\n        :param units: numbers of neural units\n        :param activation: activation function\n        :param learning_rate: learning rate for paras\n        :param is_input_layer: whether it is input layer or not\n        &quot;&quot;&quot;</span>\n        self.units = units\n        self.weight = <span class=\"hljs-literal\">None</span>\n        self.bias = <span class=\"hljs-literal\">None</span>\n        self.activation = activation\n        <span class=\"hljs-keyword\">if</span> learning_rate <span class=\"hljs-keyword\">is</span> <span class=\"hljs-literal\">None</span>:\n            learning_rate = <span class=\"hljs-number\">0.3</span>\n        self.learn_rate = learning_rate\n        self.is_input_layer = is_input_layer\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">initializer</span>(<span class=\"hljs-params\">self, back_units</span>):\n        self.weight = np.asmatrix(np.random.normal(<span class=\"hljs-number\">0</span>, <span class=\"hljs-number\">0.5</span>, (self.units, back_units)))\n        self.bias = np.asmatrix(np.random.normal(<span class=\"hljs-number\">0</span>, <span class=\"hljs-number\">0.5</span>, self.units)).T\n        <span class=\"hljs-keyword\">if</span> self.activation <span class=\"hljs-keyword\">is</span> <span class=\"hljs-literal\">None</span>:\n            self.activation = sigmoid\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">cal_gradient</span>(<span class=\"hljs-params\">self</span>):\n        <span class=\"hljs-comment\"># activation function may be sigmoid or linear</span>\n        <span class=\"hljs-keyword\">if</span> self.activation == sigmoid:\n            gradient_mat = np.dot(self.output, (<span class=\"hljs-number\">1</span> - self.output).T)\n            gradient_activation = np.diag(np.diag(gradient_mat))\n        <span class=\"hljs-keyword\">else</span>:\n            gradient_activation = <span class=\"hljs-number\">1</span>\n        <span class=\"hljs-keyword\">return</span> gradient_activation\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">forward_propagation</span>(<span class=\"hljs-params\">self, xdata</span>):\n        self.xdata = xdata\n        <span class=\"hljs-keyword\">if</span> self.is_input_layer:\n            <span class=\"hljs-comment\"># input layer</span>\n            self.wx_plus_b = xdata\n            self.output = xdata\n            <span class=\"hljs-keyword\">return</span> xdata\n        <span class=\"hljs-keyword\">else</span>:\n            self.wx_plus_b = np.dot(self.weight, self.xdata) - self.bias\n            self.output = self.activation(self.wx_plus_b)\n            <span class=\"hljs-keyword\">return</span> self.output\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">back_propagation</span>(<span class=\"hljs-params\">self, gradient</span>):\n        gradient_activation = self.cal_gradient()  <span class=\"hljs-comment\"># i * i 维</span>\n        gradient = np.asmatrix(np.dot(gradient.T, gradient_activation))\n\n        self._gradient_weight = np.asmatrix(self.xdata)\n        self._gradient_bias = -<span class=\"hljs-number\">1</span>\n        self._gradient_x = self.weight\n\n        self.gradient_weight = np.dot(gradient.T, self._gradient_weight.T)\n        self.gradient_bias = gradient * self._gradient_bias\n        self.gradient = np.dot(gradient, self._gradient_x).T\n        <span class=\"hljs-comment\"># upgrade: the Negative gradient direction</span>\n        self.weight = self.weight - self.learn_rate * self.gradient_weight\n        self.bias = self.bias - self.learn_rate * self.gradient_bias.T\n        <span class=\"hljs-comment\"># updates the weights and bias according to learning rate (0.3 if undefined)</span>\n        <span class=\"hljs-keyword\">return</span> self.gradient\n\n\n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\">BPNN</span>:\n    <span class=\"hljs-string\">&quot;&quot;&quot;\n    Back Propagation Neural Network model\n    &quot;&quot;&quot;</span>\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\">self</span>):\n        self.layers = []\n        self.train_mse = []\n        self.fig_loss = plt.figure()\n        self.ax_loss = self.fig_loss.add_subplot(<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">1</span>)\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">add_layer</span>(<span class=\"hljs-params\">self, layer</span>):\n        self.layers.append(layer)\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">build</span>(<span class=\"hljs-params\">self</span>):\n        <span class=\"hljs-keyword\">for</span> i, layer <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">enumerate</span>(self.layers[:]):\n            <span class=\"hljs-keyword\">if</span> i &lt; <span class=\"hljs-number\">1</span>:\n                layer.is_input_layer = <span class=\"hljs-literal\">True</span>\n            <span class=\"hljs-keyword\">else</span>:\n                layer.initializer(self.layers[i - <span class=\"hljs-number\">1</span>].units)\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">summary</span>(<span class=\"hljs-params\">self</span>):\n        <span class=\"hljs-keyword\">for</span> i, layer <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">enumerate</span>(self.layers[:]):\n            <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">&quot;------- layer %d -------&quot;</span> % i)\n            <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">&quot;weight.shape &quot;</span>, np.shape(layer.weight))\n            <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">&quot;bias.shape &quot;</span>, np.shape(layer.bias))\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">train</span>(<span class=\"hljs-params\">self, xdata, ydata, train_round, accuracy</span>):\n        self.train_round = train_round\n        self.accuracy = accuracy\n\n        self.ax_loss.hlines(self.accuracy, <span class=\"hljs-number\">0</span>, self.train_round * <span class=\"hljs-number\">1.1</span>)\n\n        x_shape = np.shape(xdata)\n        <span class=\"hljs-keyword\">for</span> round_i <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(train_round):\n            all_loss = <span class=\"hljs-number\">0</span>\n            <span class=\"hljs-keyword\">for</span> row <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\">range</span>(x_shape[<span class=\"hljs-number\">0</span>]):\n                _xdata = np.asmatrix(xdata[row, :]).T\n                _ydata = np.asmatrix(ydata[row, :]).T\n\n                <span class=\"hljs-comment\"># forward propagation</span>\n                <span class=\"hljs-keyword\">for</span> layer <span class=\"hljs-keyword\">in</span> self.layers:\n                    _xdata = layer.forward_propagation(_xdata)\n\n                loss, gradient = self.cal_loss(_ydata, _xdata)\n                all_loss = all_loss + loss\n\n                <span class=\"hljs-comment\"># back propagation: the input_layer does not upgrade</span>\n                <span class=\"hljs-keyword\">for</span> layer <span class=\"hljs-keyword\">in</span> self.layers[:<span class=\"hljs-number\">0</span>:-<span class=\"hljs-number\">1</span>]:\n                    gradient = layer.back_propagation(gradient)\n\n            mse = all_loss / x_shape[<span class=\"hljs-number\">0</span>]\n            self.train_mse.append(mse)\n\n            self.plot_loss()\n\n            <span class=\"hljs-keyword\">if</span> mse &lt; self.accuracy:\n                <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">&quot;----达到精度----&quot;</span>)\n                <span class=\"hljs-keyword\">return</span> mse\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">cal_loss</span>(<span class=\"hljs-params\">self, ydata, ydata_</span>):\n        self.loss = np.<span class=\"hljs-built_in\">sum</span>(np.power((ydata - ydata_), <span class=\"hljs-number\">2</span>))\n        self.loss_gradient = <span class=\"hljs-number\">2</span> * (ydata_ - ydata)\n        <span class=\"hljs-comment\"># vector (shape is the same as _ydata.shape)</span>\n        <span class=\"hljs-keyword\">return</span> self.loss, self.loss_gradient\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">plot_loss</span>(<span class=\"hljs-params\">self</span>):\n        <span class=\"hljs-keyword\">if</span> self.ax_loss.lines:\n            self.ax_loss.lines.remove(self.ax_loss.lines[<span class=\"hljs-number\">0</span>])\n        self.ax_loss.plot(self.train_mse, <span class=\"hljs-string\">&quot;r-&quot;</span>)\n        plt.ion()\n        plt.xlabel(<span class=\"hljs-string\">&quot;step&quot;</span>)\n        plt.ylabel(<span class=\"hljs-string\">&quot;loss&quot;</span>)\n        plt.show()\n        plt.pause(<span class=\"hljs-number\">0.1</span>)\n\n\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">example</span>():\n    x = np.random.randn(<span class=\"hljs-number\">10</span>, <span class=\"hljs-number\">10</span>)\n    y = np.asarray(\n        [\n            [<span class=\"hljs-number\">0.8</span>, <span class=\"hljs-number\">0.4</span>],\n            [<span class=\"hljs-number\">0.4</span>, <span class=\"hljs-number\">0.3</span>],\n            [<span class=\"hljs-number\">0.34</span>, <span class=\"hljs-number\">0.45</span>],\n            [<span class=\"hljs-number\">0.67</span>, <span class=\"hljs-number\">0.32</span>],\n            [<span class=\"hljs-number\">0.88</span>, <span class=\"hljs-number\">0.67</span>],\n            [<span class=\"hljs-number\">0.78</span>, <span class=\"hljs-number\">0.77</span>],\n            [<span class=\"hljs-number\">0.55</span>, <span class=\"hljs-number\">0.66</span>],\n            [<span class=\"hljs-number\">0.55</span>, <span class=\"hljs-number\">0.43</span>],\n            [<span class=\"hljs-number\">0.54</span>, <span class=\"hljs-number\">0.1</span>],\n            [<span class=\"hljs-number\">0.1</span>, <span class=\"hljs-number\">0.5</span>],\n        ]\n    )\n    model = BPNN()\n    <span class=\"hljs-keyword\">for</span> i <span class=\"hljs-keyword\">in</span> (<span class=\"hljs-number\">10</span>, <span class=\"hljs-number\">20</span>, <span class=\"hljs-number\">30</span>, <span class=\"hljs-number\">2</span>):\n        model.add_layer(DenseLayer(i))\n    model.build()\n    model.summary()\n    model.train(xdata=x, ydata=y, train_round=<span class=\"hljs-number\">100</span>, accuracy=<span class=\"hljs-number\">0.01</span>)\n\n\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"hljs-string\">&quot;__main__&quot;</span>:\n    example()\n"
    }
  },
  "contributors": [
    {
      "name": "Yash Bhardwaj",
      "email": "yashbhardwaj911@gmail.com",
      "commits": 1
    },
    {
      "name": "William Zhang",
      "email": "39932068+WilliamHYZhang@users.noreply.github.com",
      "commits": 1
    },
    {
      "name": "Christian Clauss",
      "email": "cclauss@me.com",
      "commits": 2
    }
  ],
  "explanationUrl": {}
}
